---
title: "NLP"
author: "Charles Lang"
---

## Libraries
```{r}
#Make sure you install and load the following libraries
library(rlang)
library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2) ## update "scale" and "ggplot2"
library(tidyverse) #You will need the full tidyverse package not tidyr and dyplr separately
library(topicmodels)

#IF USING A MAC PLEASE RUN THIS CODE
#Sys.setlocale("LC_ALL", "C")
```

## In the class-notes folder you will find real csv files exported from real student's note taking in this class. Import all document files and the list of weeks file
## Import all document files and the list of weeks file
```{r}
D1 <- list.files(path = "/Users/arvind tomar/Documents/natural-language-processing-master/class-notes",
               pattern = "*.csv", 
               full.names = T) %>% 
    map_df(~read_csv(., col_types = cols(.default = "c")))
#import list of week file
D2 <- read.csv("/Users/arvind tomar/Documents/natural-language-processing-master/week-list.csv", header = TRUE)
```

## Step 1 - Clean
```{r}
#Separate out the variables of interest
D1 <- select(D1, Title, Notes)

#Remove the htlm tags from your text
D1$Notes <- gsub("<.*?>", "", D1$Notes)
D1$Notes <- gsub("nbsp", "" , D1$Notes)
D1$Notes <- gsub("nbspnbspnbsp", "" , D1$Notes)
D1$Notes <- gsub("<U+00A0><U+00A0><U+00A0>", "" , D1$Notes)

#Merge the weeks data with your notes data so that each line has a week attributed to it 
D <- left_join(D1, D2, by = 'Title')
#Also remove readings not belonging to the class (IE - that are NA for week)
Df<- D[!is.na(D$week),]
```

## Step 2 - Process text using the tm package
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <- VCorpus(VectorSource(D1$Notes))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove meaningless verbs words which have high frequencies
corpus <- tm_map(corpus, removeWords,c("can","make","also","use","one","two","way","may","like","will","need"))
#Convert to plain text for mapping by wordcloud package
corpus <- tm_map(corpus, PlainTextDocument, lazy = TRUE)

#Convert corpus to a term document matrix - so each word can be analyzed individuallly
tdm.corpus <- TermDocumentMatrix(corpus)

#Note: we won't remove plural words here, plural words in English tend to be highly irregular and difficult to extract reliably
```
####################################################
* First, I cleaned the text using tm_map as original data is raw and contain too much unnecessary data 
* white spaces, numbers and punctuation were remove since they aren't part of text corpus
* text is converted in lower case, so that lower case and upper case of same word aren't treated differently
* Removed pre-defined stop words such as "the" and "a", as they don't have significant value in text corpus
* Stemming is the process of reducing a word to its root word. It reduces the processing time and enable comparison of the word profiles across documents. Words like "education", "educate", and "educating" can be recorded as "edu" instead of having all of three in our text corpus. 

Other steps, I would take to process the text before analyzing are followings:
* Removing helping verbs such as "is", "can", "may" etc.
* remove meaningless special character like #,@,! etc and replace meaningful symbols such as $ to "dollar" word
*Replace abbreviations with their full text equivalents (e.g. “Dr” becomes “doctor”)
* remove meaningless words which have high frequencies from the documents. here it is "make", "also","use", "can" etc
###################################################


## Step 3 - Find common words
```{r}
#The tm package can do some simple analysis, like find the most common words
findFreqTerms(tdm.corpus, lowfreq=500, highfreq=Inf)
#We can also create a vector of the word frequencies that can be useful to see common and uncommon words
word.count <- sort(rowSums(as.matrix(tdm.corpus)), decreasing=TRUE)
word.count <- data.frame(word.count)
#Look at the word.count dataframe
```

## Generate a Word Cloud

### ColorBrewer
ColorBrewer is a useful tool to help you choose colors for visualizations that was originally built for cartographers. On the ColorBrewer website (http://colorbrewer2.org/#) you can test different color schemes or see what their preset color schemes look like. This is very useful, especially if you are making images for colorblind individuals. 
```{r}
#Define the colors the cloud will use
col=brewer.pal(6,"Dark2")
#Generate cloud, make sure your window is large enough to see it
wordcloud(corpus, min.freq=80, scale=c(5,2),rot.per = 0.25,
          random.color=T, max.word=45, random.order=F,colors=col)
## scale controls the size of the words (font), and max.words limits the maximum number of words to be plotted, rot.per is the proportion of words with 90 degree rotation (vertical text), random.colors choose colors randomly from the colors
```

# Sentiment Analysis

## Match words in corpus to lexicons of positive & negative words
```{r}
#Upload positive and negative word lexicons
positive <- readLines("positive-words.txt")
negative <- readLines("negative-words.txt")

#Search for matches between each word and the two lexicons
D1$positive <- tm_term_score(tdm.corpus, positive)
D1$negative <- tm_term_score(tdm.corpus, negative)

#Generate an overall pos-neg score for each line
D1$score <- D1$positive - D1$negative

```

## Merge with week list to have a varibale representing weeks for each entry

## we have already created merge data with week list previously as Df. We will use df data to create term document matrix and sentiment analysis

## We will repeat the same steps, we have done previously with Df data

```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus1 <- VCorpus(VectorSource(Df$Notes))
#Remove spaces
corpus1 <- tm_map(corpus1, stripWhitespace)
#Convert to lower case
corpus1 <- tm_map(corpus1, content_transformer(tolower)) 
#Remove pre-defined stop words ('the', 'a', etc)
corpus1 <- tm_map(corpus1, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") 
corpus1 <- tm_map(corpus1, stemDocument)
#Remove numbers
corpus1 <- tm_map(corpus1, removeNumbers, lazy=TRUE)
#remove punctuation
corpus1 <- tm_map(corpus1, removePunctuation, lazy=TRUE)
#Convert corpus to a term document matrix
tdm.corpus1 <- TermDocumentMatrix(corpus1)
#Define the colors the cloud will use
col=brewer.pal(6,"Dark2")
#Generate cloud
wordcloud(corpus1, min.freq=80, scale=c(5,2),rot.per = 0.25,
          random.color=T, max.word=45, random.order=F,colors=col)
```

## Match words in corpus to lexicons of positive & negative words
```{r}
#Upload positive and negative word lexicons
positive <- readLines("positive-words.txt")
negative <- readLines("negative-words.txt")

#Search for matches between each word and the two lexicons
Df$positive <- tm_term_score(tdm.corpus1, positive)
Df$negative <- tm_term_score(tdm.corpus1, negative)

#Generate an overall pos-neg score for each line
Df$score <- Df$positive - Df$negative

```


## Using ggplot Generate a visualization of the mean sentiment score over weeks, remove rows that have readings from other classes (NA for weeks). You will need to summarize your data to achieve this.
```{r}
Df1 <- select(Df, week, score)
Df2 <- Df1 %>% 
    group_by(week) %>% 
    summarise(sentiment_score = sum(score))
pdf("Sentiment_score_VS_weeks.pdf")
ggplot(data=Df2, aes(x=week, y=sentiment_score)) + geom_col(fill="red")
```

# LDA Topic Modelling

Using the same csv file you have generated the LDA analysis will treat each row of the data frame as a document. Does this make sense for generating topics?

```{r}
#Term Frequency Inverse Document Frequency
dtm.tfi <- DocumentTermMatrix(corpus, control = list(weighting = weightTf))

#Remove very uncommon terms (term freq inverse document freq < 0.1)
dtm.tfi <- dtm.tfi[,dtm.tfi$v >= 0.1]

#Remove non-zero entries
rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
dtm.tfi2   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows

#Identify rows with zero entries
Zero_rows<-as.vector(which(rowTotals %in% c(0)))

#Remove these rows from original dataset
D1 <- D1[-Zero_rows,]

#Generate LDA model, k is the number of topics and the seed is a random number to start the process
lda.model = LDA(dtm.tfi2, k = 5, seed = 150)

#Which terms are most common in each topic
terms(lda.model,k=5) 

#Identify which documents belong to which topics based on the notes taken by the student
D1$topic <- topics(lda.model)

```

What does an LDA topic represent? 
# LDA stands for Latent Dirichlet Allocation and is a type of topic modelling algorithm. It is used to learn the representation of a fixed number of topics.
#An LDA topic represents the probability of a word appearing in a document of a certain topic

# Final Task 

Find a set of documents, perhaps essays you have written or articles you have available and complete an LDA analysis of those documents. Does the method group documents as you would expect?

```{r}
# reading text documents

file1<-"/Users/arvind tomar/Documents/natural-language-processing-master/America_Your_Privacy_Settings_Are_All_Wrong.txt"
doc1<- readChar(file1, file.info(file1)$size)
file2<-"/Users/arvind tomar/Documents/natural-language-processing-master/Desperate Times Creative Measures.txt"
doc2<-readChar(file2, file.info(file2)$size)
file3<-"/Users/arvind tomar/Documents/natural-language-processing-master/Many Jobless Workers Arent Getting Help.txt"
doc3<-readChar(file3, file.info(file3)$size)
# create a dataFrame for the documents and put name of the document as Title and content as Notes

Titles<-c("America, Your Privacy Settings Are All Wrong","Desperate Times, Creative Measures","Many Jobless Workers Aren’t Getting Help")
Notes<-c(doc1,doc2,doc3)
Doc<- data.frame("Titles"=Titles,"Notes"=Notes,stringsAsFactors = FALSE)

```


```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus2 <- VCorpus(VectorSource(Doc$Notes))
#Remove spaces
corpus2 <- tm_map(corpus2, stripWhitespace)
#Convert to lower case
corpus2 <- tm_map(corpus2, content_transformer(tolower)) 
#Remove pre-defined stop words ('the', 'a', etc)
corpus2 <- tm_map(corpus2, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") 
corpus2 <- tm_map(corpus2, stemDocument)
#Remove numbers
corpus2 <- tm_map(corpus2, removeNumbers, lazy=TRUE)
#remove punctuation
corpus2 <- tm_map(corpus2, removePunctuation, lazy=TRUE)
corpus2 <- tm_map(corpus2, removeWords,c("can","make","also","use","one","two","way","may","like","will","need","even"))
#Convert corpus to a term document matrix
tdm.corpus2 <- TermDocumentMatrix(corpus2)
#Define the colors the cloud will use
col=brewer.pal(6,"Dark2")
#Generate cloud
wordcloud(corpus2, min.freq=80, scale=c(5,2),rot.per = 0.25,
          random.color=T, max.word=45, random.order=F,colors=col)
```

```{r}
#Term Frequency Inverse Document Frequency
dtm.tfi2 <- DocumentTermMatrix(corpus2, control = list(weighting = weightTf))

#Remove very uncommon terms (term freq inverse document freq < 0.1)
dtm.tfi2 <- dtm.tfi2[,dtm.tfi2$v >= 0.1]

#Remove non-zero entries
#rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
#dtm.tfi2   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows

#Identify rows with zero entries
#Zero_rows<-as.vector(which(rowTotals %in% c(0)))

#Remove these rows from original dataset
#D1 <- D1[-Zero_rows,]

#Generate LDA model, k is the number of topics and the seed is a random number to start the process
lda.model2 = LDA(dtm.tfi2, k = 3, seed = 150)

#Which terms are most common in each topic
terms(lda.model2,k=10) 

#Identify which documents belong to which topics based on the notes taken by the student
Doc$topic <- topics(lda.model2)

```
